{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unpack ETL files",
   "id": "f8fab94252f7ab8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:47.248587Z",
     "start_time": "2025-10-12T17:32:45.913383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from openai.types.beta.threads import image_file\n",
    "\n",
    "project_root = Path().resolve()\n",
    "etl_dir = project_root / \"data\" / \"ETL8G\"\n",
    "unpack_script = project_root / \"data\" / \"unpack_etlcdb\" / \"unpack_etlcdb\" / \"unpack.py\"\n",
    "\n",
    "files = os.listdir(etl_dir)\n",
    "\n",
    "for file in files:\n",
    "    if file != \"ETL8INFO\":\n",
    "        input_file = etl_dir / file\n",
    "        cmd = f'python {unpack_script} {input_file}'\n",
    "        print(\"Running:\", cmd)\n",
    "        # os.system(cmd)\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_01\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_01_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_02\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_02_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_03\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_03_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_04\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_04_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_05\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_05_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_06\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_06_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_07\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_07_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_08\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_08_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_09\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_09_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_10\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_10_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_11\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_11_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_12\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_12_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_13\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_13_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_14\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_14_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_15\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_15_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_16\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_16_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_17\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_17_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_18\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_18_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_19\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_19_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_20\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_20_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_21\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_21_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_22\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_22_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_23\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_23_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_24\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_24_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_25\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_25_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_26\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_26_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_27\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_27_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_28\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_28_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_29\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_29_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_30\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_30_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_31\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_31_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_32\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_32_unpack\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_33\n",
      "Running: python C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\unpack_etlcdb\\unpack_etlcdb\\unpack.py C:\\Users\\alicj\\PycharmProjects\\KanjiRecognitionModel\\data\\ETL8G\\ETL8G_33_unpack\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving labels to list",
   "id": "7f6d9ff940d65629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:50.163710Z",
     "start_time": "2025-10-12T17:32:47.255582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_labels = Path().resolve()/ \"data\" / \"ETL8G\"/ \"ETL8G_01_unpack\"/\"meta.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(path_to_labels)\n",
    "\n",
    "labels = labels_df[\"char\"]\n",
    "labels = labels.tolist()\n"
   ],
   "id": "e81ffe2bfd5e0650",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:50.819612Z",
     "start_time": "2025-10-12T17:32:50.803209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_labels = labels.copy()\n",
    "labels_copy = all_labels.copy()\n",
    "\n",
    "for i in range(31):\n",
    "    labels_copy_2 = labels_copy.copy()\n",
    "    all_labels.extend(labels_copy_2)\n",
    "\n",
    "all_labels.extend(labels_copy[:956])"
   ],
   "id": "6d26776685765acf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:50.856308Z",
     "start_time": "2025-10-12T17:32:50.845057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "#\n",
    "# le = LabelEncoder()\n",
    "# labels_int = le.fit_transform(labels)\n",
    "#\n",
    "# print(labels_int[:10])        # now numbers like [0, 1, 0, 2, ...]\n",
    "# print(le.classes_[:10])       # the mapping back to original labels\n"
   ],
   "id": "c7dbfc3c177523e3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:50.910088Z",
     "start_time": "2025-10-12T17:32:50.904811Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ce0b07193c4aad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:53.951033Z",
     "start_time": "2025-10-12T17:32:50.923868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# go through each folder\n",
    "path_label_list = []\n",
    "etl_dir = Path().resolve() / \"data\" / \"ETL8G\"\n",
    "for folder in os.listdir(etl_dir):\n",
    "    if \"unpack\" in folder:\n",
    "        folder_path = os.path.join(etl_dir, folder)\n",
    "\n",
    "        # go through each png in folder\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if fname.endswith(\".png\"):\n",
    "                fpath = os.path.join(folder_path, fname)\n",
    "                # take filename without extension\n",
    "                idx = int(os.path.splitext(fname)[0])\n",
    "                # compute label\n",
    "                label = idx % 956\n",
    "                path_label_list.append((fpath, label))\n",
    "\n",
    "print(path_label_list[956])\n"
   ],
   "id": "80d1bbda49220738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\alicj\\\\PycharmProjects\\\\KanjiRecognitionModel\\\\data\\\\ETL8G\\\\ETL8G_01_unpack\\\\00956.png', 0)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:53.980216Z",
     "start_time": "2025-10-12T17:32:53.973224Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(path_label_list))",
   "id": "eb7d4e46c0e8301a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153916\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:54.122898Z",
     "start_time": "2025-10-12T17:32:54.018474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(labels[1])\n",
    "print(path_label_list[1912])"
   ],
   "id": "dc1729b2fcd018e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "愛\n",
      "('C:\\\\Users\\\\alicj\\\\PycharmProjects\\\\KanjiRecognitionModel\\\\data\\\\ETL8G\\\\ETL8G_01_unpack\\\\01912.png', 0)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving images",
   "id": "c90cef8814ea4d48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:32:55.325657Z",
     "start_time": "2025-10-12T17:32:54.157167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_paths = []\n",
    "img_dir_path = Path().resolve()/ \"data\" / \"ETL8G\"\n",
    "\n",
    "files = os.listdir(img_dir_path)\n",
    "for file in files:\n",
    "    if \"unpack\" in file:\n",
    "        unpack_dir = img_dir_path / file\n",
    "        img_files = os.listdir(unpack_dir)\n",
    "        for img_file in img_files:\n",
    "            if img_file != 'meta.csv':\n",
    "                img_path = unpack_dir / img_file\n",
    "                image_paths.append(img_path)\n",
    "\n",
    "# in the last dir there are only 955 images\n",
    "\n"
   ],
   "id": "96a367410b25f080",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:33:00.986892Z",
     "start_time": "2025-10-12T17:33:00.979690Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(labels))",
   "id": "be43b50735ec8f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4780\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset class",
   "id": "368a45cb69af6690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:54:25.717131500Z",
     "start_time": "2025-10-12T17:33:03.426503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = self.data[idx]\n",
    "        # path = item[0]\n",
    "        # img = Image.open(path)\n",
    "        # img=img.convert(\"L\")\n",
    "        # img = T.ToTensor()(img)\n",
    "        # label =item[1]\n",
    "        # # img.show()\n",
    "        # return img, torch.tensor(label, dtype=torch.long)\n",
    "        path, label = self.data[idx]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.ToTensor()(img)\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n"
   ],
   "id": "2e1e0e40dfe36b6e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:54:25.717131500Z",
     "start_time": "2025-10-12T17:33:11.569244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "a = ImageDataset(path_label_list, transform)\n",
    "img, lab = a[0]\n",
    "# print(img.size)\n",
    "print(lab)\n",
    "print(type(img), img.shape, lab)"
   ],
   "id": "ade7c40ecfcc3ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "<class 'torch.Tensor'> torch.Size([1, 64, 64]) tensor(0)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting the data into train, val, test sets",
   "id": "92b1874541ceaf8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:33:24.120013Z",
     "start_time": "2025-10-12T17:33:16.092040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# first split train vs temp (val+test)\n",
    "train_data, temp_data = train_test_split(path_label_list, test_size=0.3, random_state=42, stratify=[d[1] for d in path_label_list]) # used tratify to preserve the same class proportions\n",
    "\n",
    "# then split temp into val and test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=[d[1] for d in temp_data])\n"
   ],
   "id": "526c15f1445562f2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:33:54.542528Z",
     "start_time": "2025-10-12T17:33:54.536042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ImageDataset(train_data, transform)\n",
    "val_dataset   = ImageDataset(val_data, transform)\n",
    "test_dataset  = ImageDataset(test_data, transform)\n"
   ],
   "id": "dae3c9ca35918301",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loader",
   "id": "b75ff2385de8a8bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:52:07.973319Z",
     "start_time": "2025-10-12T17:52:07.962430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, num_workers=0)\n"
   ],
   "id": "66a2aab255130944",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TEST",
   "id": "c362e32410df6be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:52:10.478211Z",
     "start_time": "2025-10-12T17:52:10.408093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imgs, labels = next(iter(train_loader))\n",
    "print(imgs.shape)    # [32, 1, 64, 64]\n",
    "print(labels.shape)  # [32]\n"
   ],
   "id": "c8a694d7366381aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:18:57.650588Z",
     "start_time": "2025-10-12T18:18:57.501769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# 1) Build a proper Dataset from your (path, label) list\n",
    "transform = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.ToTensor(),                 # [0,1], shape [1,64,64] (your dataset .convert(\"L\") already makes grayscale)\n",
    "    T.Normalize([0.5], [0.5]),\n",
    "])\n",
    "dataset = ImageDataset(path_label_list, transform=transform)\n",
    "\n",
    "# 2) Take a random subset of 1000 items\n",
    "subset_size = min(2000, len(dataset))    # just in case you have < 1000\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "small_dataset = Subset(dataset, indices)\n",
    "\n",
    "# 3) DataLoader from the subset (num_workers=0 is safest in notebooks)\n",
    "train_loader = DataLoader(small_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, num_workers=0)\n",
    "\n",
    "# 4) Sanity check: can we get one batch?\n",
    "imgs, labels = next(iter(train_loader))\n",
    "print(imgs.shape, labels.shape)  # expect: torch.Size([64, 1, 64, 64]) and torch.Size([64])\n"
   ],
   "id": "1e5edccd6eaf6b87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:18:59.993515Z",
     "start_time": "2025-10-12T18:18:59.839060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Loaded one batch in {time.time() - t0:.2f} seconds\")\n",
    "print(\"Batch shape:\", images.shape, \"Labels:\", labels.shape)\n"
   ],
   "id": "2ca2765702014800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded one batch in 0.14 seconds\n",
      "Batch shape: torch.Size([64, 1, 64, 64]) Labels: torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:19:00.243209Z",
     "start_time": "2025-10-12T18:19:00.235138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 8, kernel_size=3, padding=1)  # input: 1x64x64 → output: 8x64x64\n",
    "        self.pool = nn.MaxPool2d(2, 2)                         # → 8x32x32\n",
    "        self.fc   = nn.Linear(8 * 32 * 32, num_classes)        # flatten → fully connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))  # conv → relu → pool\n",
    "        x = x.view(x.size(0), -1)            # flatten\n",
    "        x = self.fc(x)                       # final layer\n",
    "        return x\n"
   ],
   "id": "68f15bddbd59084c",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:19:02.085174Z",
     "start_time": "2025-10-12T18:19:02.077154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Compute accuracy of a model on a given DataLoader.\"\"\"\n",
    "    model.eval()                          # evaluation mode (no dropout, batchnorm updates)\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in loader:\n",
    "        outputs = model(images)            # forward pass\n",
    "        preds = outputs.argmax(1)          # highest logit = predicted class\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "    acc = correct / total\n",
    "    return acc\n"
   ],
   "id": "7c708a3f6fe42be6",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:24:03.156371Z",
     "start_time": "2025-10-12T18:19:11.771317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === setup ===\n",
    "device = torch.device(\"cpu\")   # change to \"cuda\" if you fix your GPU\n",
    "num_classes = 956              # for example\n",
    "\n",
    "model = TinyCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# assume you already have a DataLoader\n",
    "# for example: train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# === training loop ===\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # move to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train loss = {avg_loss:.4f}, Val acc = {val_acc*100:.2f}%\")\n",
    "    # print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"✅ Training complete!\")"
   ],
   "id": "bc8f8f1e3f4e8821",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss = 13.0760, Val acc = 0.10%\n",
      "Epoch 2: Train loss = 7.9223, Val acc = 0.10%\n",
      "Epoch 3: Train loss = 6.9096, Val acc = 0.22%\n",
      "Epoch 4: Train loss = 6.4128, Val acc = 0.26%\n",
      "Epoch 5: Train loss = 5.6046, Val acc = 0.24%\n",
      "Epoch 6: Train loss = 4.4888, Val acc = 0.30%\n",
      "Epoch 7: Train loss = 3.4959, Val acc = 0.29%\n",
      "Epoch 8: Train loss = 2.5908, Val acc = 0.16%\n",
      "Epoch 9: Train loss = 1.8387, Val acc = 0.18%\n",
      "Epoch 10: Train loss = 1.3071, Val acc = 0.16%\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test CNN",
   "id": "390a53b0fb04c2a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:24:51.346786Z",
     "start_time": "2025-09-16T21:24:51.341518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 1 input channel (grayscale), 8 filters\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)   # reduces size by half\n",
    "        # Flattened size after pool (assuming 64x64 input): 8 * 32 * 32\n",
    "        self.fc1 = nn.Linear(32256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → relu → pool\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = self.fc1(x)                       # fully connected\n",
    "        return x\n"
   ],
   "id": "ec0597d2bcf13537",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T17:37:15.175251Z",
     "start_time": "2025-10-12T17:37:15.116230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: suppose you have 100 kanji classes\n",
    "model = SimpleCNN(num_classes=956)\n",
    "\n",
    "# One fake batch: 32 grayscale images, 64x64 each\n",
    "x = torch.randn(32, 1, 64, 64)\n",
    "out = model(x)\n",
    "\n",
    "print(out.shape)  # torch.Size([32, 100]) → 32 predictions, 100 classes\n"
   ],
   "id": "b533d0162f988710",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Example: suppose you have 100 kanji classes\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model = \u001B[43mSimpleCNN\u001B[49m(num_classes=\u001B[32m956\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# One fake batch: 32 grayscale images, 64x64 each\u001B[39;00m\n\u001B[32m      5\u001B[39m x = torch.randn(\u001B[32m32\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m64\u001B[39m, \u001B[32m64\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'SimpleCNN' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:31:20.817794Z",
     "start_time": "2025-09-16T21:25:38.246830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):  # 3 training epochs\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()         # reset gradients\n",
    "        outputs = model(images)       # forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()               # backward pass\n",
    "        optimizer.step()              # update weights\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ],
   "id": "5d4db267150c3e75",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[138]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[32m      7\u001B[39m     optimizer.zero_grad()         \u001B[38;5;66;03m# reset gradients\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m       \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[32m      9\u001B[39m     loss = criterion(outputs, labels)\n\u001B[32m     10\u001B[39m     loss.backward()               \u001B[38;5;66;03m# backward pass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[134]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mSimpleCNN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.pool(F.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))  \u001B[38;5;66;03m# conv → relu → pool\u001B[39;00m\n\u001B[32m     16\u001B[39m     x = x.view(x.size(\u001B[32m0\u001B[39m), -\u001B[32m1\u001B[39m)             \u001B[38;5;66;03m# flatten\u001B[39;00m\n\u001B[32m     17\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.fc1(x)                       \u001B[38;5;66;03m# fully connected\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m548\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    533\u001B[39m         F.pad(\n\u001B[32m    534\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    541\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    542\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53b800e396bdd5b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
